---
layout:     post
title:      模型超参数优化
subtitle:   Optimization of Hyperparameters
date:       2018-07-09
author:     xhhszc
catalog: true
tags:
    - Machine Learning
    - Hyperopt
---

模型超參优化
---------
## 1. 使用hyperopt进行参数优化
### 1.1 安装hyperopt
hyperopt网址：http://hyperopt.github.io/hyperopt/

安装方法： pip install hyperopt

NOTE：我在安装后运行程序时出现了错误：
```python
File "/home/michael/work/oanda/src/oanda/trend_prediction/find_optimal_model.py", line 124, in <module> main() 
File "/home/michael/work/oanda/src/oanda/trend_prediction/find_optimal_model.py", line 116, in main trials=trials)
File "/home/michael/work/oanda/runtime/lib/python3.5/site-packages/hyperopt/fmin.py", line 307, in fmin return_argmin=return_argmin,
File "/home/michael/work/oanda/runtime/lib/python3.5/site-packages/hyperopt/base.py", line 635, in fmin return_argmin=return_argmin) 
File "/home/michael/work/oanda/runtime/lib/python3.5/site-packages/hyperopt/fmin.py", line 314, in fmin pass_expr_memo_ctrl=pass_expr_memo_ctrl) 
File "/home/michael/work/oanda/runtime/lib/python3.5/site-packages/hyperopt/base.py", line 786, in __init__ pyll.toposort(self.expr) 
File "/home/michael/work/oanda/runtime/lib/python3.5/site-packages/hyperopt/pyll/base.py", line 715, in toposort assert order[-1] == expr

TypeError: 'generator' object is not subscriptable
```
解决方案：Hyperopt不支持networkx-2.0,我换成了1.11版本就好了

### 1.2 使用方法
hyperopt实际上只是提供了一个优化的接口，因此我们可以利用这个接口实现我们任何模型的参数优化，包括Tensorflow或sklearn等python实现的模型。

####  1.  以sklearn中的决策树回归（DecisionTreeRegressor）为例：
```python
from sklearn.tree import DecisionTreeRegressor #需要调參的模型

from sklearn.cross_validation import cross_val_score # 交叉验证

from hyperopt import fmin, tpe, hp, rand #参数优化
```

####    2.  引入相关的包之后，我们需要定义模型函数：
其输入应该为模型的超参数（argsDict），输出为该模型的评分，这个评分应该要满足“越小越好”的条件
![definemodel](https://img-blog.csdn.net/20180709185307203?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3hoaHN6Yw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)


argsDict是一个参数的字典， dtr是利用argsDict实例化DecisionTressRegressor的模型，

metric是利用训练集x_train(训练集的特征)和y_train（训练集的标签），（x_train 和 y_train都是全局变量，这里不再赘述）进行交叉验证得到的score的均值

score是我们定义的scoreing=“neg_mean_absolute_error”

需要注意的是，“neg_mean_absolute_error”是MAE的负数，我们的优化目标是使MAE越小越好， 因此应该返回“-metric”

另外 交叉验证并不是必须的，只要return的是这个模型的一个评分就行。

####    3.  在主函数中定义好你的搜索域、利用fmin进行搜索
![fmin](https://img-blog.csdn.net/20180709185404911?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3hoaHN6Yw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

algo是参数优化的策略（即参数搜索算法），当前支持随机搜索（hyperopt.rand.suggest）、Tree of Parzen Estimators（hyperopt.pte.suggest）（据说是贝叶斯优化的一种变体，但没找到太靠谱的资料）

max_evals是指最大尝试的模型个数，越大越容易找到最优解

fmin返回的best是搜索到的最优参数字典

最后的clf = DTR（best, is_return=True）是我用来获取best参数实例化的模型句柄，以方便后续在测试集上的评估

### 1.3 hyperopt-sklearn
hyperopt-sklearn是给予hyperopt专门为了sklearn而封装的包，使用起来也很简单，但是目前只支持sklearn中的几种模型（见官网说明）。

网址：https://github.com/hyperopt/hyperopt-sklearn
