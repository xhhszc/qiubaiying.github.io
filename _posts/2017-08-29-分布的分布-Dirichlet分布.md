---
layout:     post
title:      分布的分布-Dirichlet分布
subtitle:   Distribution of distribution - Dirichlet distribution
date:       2017-08-29
author:     xhhszc
catalog: true
tags:
    - Machine Learning
    - Dirichlet distribution
---

分布的分布-Dirichlet分布
--------

目录
- 伯努利过程
	- 伯努利审判
	- 伯努利过程
	- 贝叶斯理论
- beta分布
- 狄利克雷Dirichlet分布
	- 中国餐馆过程
	- Polya urn模型
	- stick-breaking模型
	- 无限混合模型

------


#1 伯努利过程

## 1.1伯努利审判
只有两种结果的审判（如抛硬币）。
## 1.2伯努利过程
重复进行伯努利审判的过程（如抛n次硬币）。

我们现在用更正规的语言重新整理一下思路。现在有个硬币得到random sample X  = (x1,x2,...xn)，我们需要基于这n次观察的结果来估算一下q在[0,1]中取哪个值比较靠谱，由于我们不能再用单一一个确定的值描述q，所以我们用一个分布函数来描述：有关q的概率密度函数（说得再简单点，即是q在[0,1]“分布律”）。当然，这应当写成一个条件密度：f(q|X)，因为我们总是观测到X的情况下，来猜的q。
## 1.3 贝叶斯理论
$P(q|x)P(x)=P(X=x|q)P(q)$
注意，到这里公式里出现的都是"概率"，并没有在[0,1]上的概率密度函数出现。为了让贝叶斯定理和密度函数结合到一块。我们可以从方程两边由P(q)得到f(q)，而由P(q|x)得到f(q|x)。
$f(q|x) \sim P(X=x|q)f(q)$

连续抛n次，即为一个bernoulli process，则在q确定时，n次抛掷结果确定时，又观察得到k次字的概率可以描述为：
$P(X=x|p)=q^k(1-q)^{n-k}$
$f(q|x) \sim q^k(1-q)^{n-k}f(q)$

我们这时其实已经得到了一个求f(q|x)的公式（只要在n次观测下确定了，f(q)确定了，那么f(q|x)也确定了)。

# 2 beta 分布
现在在来看f(q)。显然，在我们对硬币一无所知的时候，我们应当认为硬币抛出字的概率q有可能在[0,1]上任意处取值。f(q)在这里取个均匀分布的密度函数是比较合适的，即f(q) = 1。

则
$f(q|x) \sim q^k(1-q)^{n-k}$

其中$q^k(1-q)^{n-k} \sim f(q)$是beta分布。
Beta distribution由两个参数alpha、beta确定；在这里对应的alpha等于k+1，beta等于n+1-k。而均匀分布的先验密度函数，就是那个f(q)也可以被beta distribution描述，这时alpha等于1，beta也等于1。
更有意思的是，当我们每多抛一次硬币，出现字时，我们只需要alpha = alpha + 1；出现头只需要beta = beta + 1。这样就能得到需要估计的概率密度f(q|x)。
其实之所以计算会变得这么简单，是因为被beta distribution描述的prior经过bayes formula前后还是一个beta distribution；这种不改变函数本身所属family的特性，叫共轭(conjugate)

# 3 狄利克雷分布

对于有两个结果的重复Bernoulli trial，我们用beta prior/distribution就能解决。那么加入我们有n个结果呢？比如抛的是骰子？
这时候上面的Bernoulli trial就要变成有一次trial有k个可能的结果； Bernoulli distribution就变成multinomial distribution。而beta distribution所表述的先验分布，也要改写成一个多结果版本的先验分布。那就是dirichlet distribution。

均匀的先验分布Beta(1,1)也要变成k个结果的Dir(alpha/K)。dirichlet prior也有共轭的性质，所以也是非常好计算的。

## 3.1 中国餐馆过程

假设一个中国餐馆有无限的桌子，第一个顾客到来之后坐在第一张桌子上。第二个顾客来到可以选择坐在第一张桌子上，也可以选择坐在一张新的桌子上，假设第n＋1个顾客到来的时候，已经有k张桌子上有顾客了，分别坐了n1,n2,...，nk个顾客，那么第n＋1个顾客可以以概率为$\frac{n_i}{\alpha+n}$坐在第i张桌子上，ni为第i张桌子上的顾客数；同时有概率为$\frac{\alpha}{\alpha+n}$选取一张新的桌子坐下。那么在n个顾客坐定之后，很显然CRP把这n个顾客分为了K个堆，即K个clusters，可以证明CRP就是一个DP。

注意这里有一个限制，每张桌子上只能有同一个dish，即一桌人喜欢吃同一道菜。

## 3.2 Polya urn模型

假设我们有一个缸，里面没有球，现在我们从一个分布H中选取一种颜色，然后把这种颜色涂在一个球上放入缸中；然后我们要么从缸中抽取一个球出来，然后再放入两个和这个球同种颜色的球进入缸中；要么就从分布H中选取一个颜色，然后把这种颜色涂在一个球上放入缸中。从缸中抽取某种颜色的一个球的概率是$\frac{n_i}{\alpha+n}$，ni是这种颜色的球的个数，n是总的球个数；不从缸中抽取而放入一种颜色的球的概率是$\frac{\alpha}{\alpha+n}$。很明显，polya urn模型和CRP有一一对应的关系，颜色对应一个桌子，坐新桌子对应于不从缸中选取而是从H中选取一种颜色涂球放入缸中。
## 3.3 stick-breaking模型
假设有一个长度为1的线段，我们从中选取$\pi_1$长出来，剩下的部分再选取$\pi_2$出来，循环下去，$\pi_n$，无穷下去，这个有点类似我们古代的一句话：

“一尺之踵，日取其半，万世不竭”，它们满足$\sum \pi_i ＝ 1$
对每个$\pi_i$，我们都从分布H中选取一个$\theta_i$，然后从$F(\theta_i)$中选取出一个$x_i$出来。这里的$\theta_i$就对应一个cluster，类似地，我们可以看到数据自然地被分为了各个堆，可以证明这个模型仍然是一个DP。
## 3.4 无限混合模型
从stick－breaking模型我们看出，我们可以把DP看着是一个无限混合模型，即

$G \sum \sum_1^\inf \pi_i*F(\theta_i)$
其中$\sum \pi_i ＝ 1$。$\pi_i $就是混合模型中每个子模型的权重。

目前应用最多的还是从第五种角度来看待问题，即把DP看着是一个无限混合模型，其中值得注意的是：

1）虽然DP是一个无限混合模型，但是可以证明，随着数据的增多，模型的个数是呈现log 增加的，即模型的个数的增长是比数据的增长要缓慢得多的；

2）DP是有一个马太效应在里面的，即越富裕的人越来越富裕，我们可以从第二和第三种解释中看到，每个桌子或者颜色已经有的数据越多，那么下一次被选中的概率越大，因为是与在桌子上的个数成正比的。